# 6 XAI

# Explainability

- What is the Saliency Map?
    - Saliency describes unique features in an image (Pixels, resolution, ..).  The purpose is to visualize the conspicuity of an image or in other words to differentiate the visual features of it.
    - This could be the color scale of an image, which is done by converting it to black and white, displaying a given temperature, or think of night vision (brightness is green, and darkness is black).

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled.png)

    Could be used for traffic light detection. (Saliency detection)

    Source: [https://analyticsindiamag.com/what-are-saliency-maps-in-deep-learning/](https://analyticsindiamag.com/what-are-saliency-maps-in-deep-learning/)

- What is Integrated Gradients?
    - Attribution Algorithm. Works by specifying the target class.
- What is the difference betweeen CNN Heat Maps: Gradients vs. DeconvNets vs. Guided Backpropagation?

    [CNN Heat Maps: Gradients vs. DeconvNets vs. Guided Backpropagation](https://glassboxmedicine.com/2019/10/06/cnn-heat-maps-gradients-vs-deconvnets-vs-guided-backpropagation/)

- What is NMF?
    - A matrix factorisation technique (another one would be SVD). It stands for Non Negative Matrix Factorization and splits the user item matrix R into the matrices U and V. The optimization problem is that the multiplication of U and V should result in R.

- What is NCF (Neural Collaborative Filtering)?
    - Models User Item interactions in the latent space with a NN
    - Is a generalization of Matrix Factorization
    - In the input layer, the user and item are one-hot encoded.
    - Neural CF Layers can be anything that is non linear

    They claim that the precendent dot product of matrix factorization limits the expressiveness of user and item latent vectors. (vector a might be closer to vector b than to c, although it is more similar to c.

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%201.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%201.png)

    Neural CF Layer is replaced by a multiplication layer, which performs element-wise multiplication on the two inputs. Weights from multiplication layer to output is a identity matrix with a linear activation function.

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%202.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%202.png)

    y_ui = predicted item of recommender matrix

    L = Linear activation function

    circle with dot = element-wise multiplicaton

    p_u = latent vector user

    q_I = latent vector item

    Since J is an identity matrix it transforms the element-wise multiplication to a dot product. Because L is a linear function it means that the input would only be up or downscaled, but there is no real transformation. Hence, we can omit it. We end up with the same function as for Matrix Factorization, which shows, that NCF can be a generalization of Matrix Factorization. 

    To leverage the architecture of NCF we need non-linearity. It includes Multiple Layer Perceptron (MLP), Generalized Matrix Factorization (GMP) and a non linear activation function sigmoid.

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%203.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%203.png)

    Source:

    [https://towardsdatascience.com/paper-review-neural-collaborative-filtering-explanation-implementation-ea3e031b7f96](https://towardsdatascience.com/paper-review-neural-collaborative-filtering-explanation-implementation-ea3e031b7f96)

- What are the components of a NeuMF (Non-linear NCF)?
    - GMF (Generalized Matrix Factorization)
    - MLP (Multi Layer Perceptron)
- What is a Multilayer perceptron network (MLP)?

    It is just a name for the simplest kind of deep neural network. 

    - There's at least one hidden layer
    - Each neuron has a non-linear activation function
    - Densely connected (each neuron in one layer is connected to every neuron the next or previous layer)
    - Uses as usual optimizer (e.g. Adam) and as a Loss function Binary cross-entroy for example
- What is the Mean Reciprocal Rank (MRR)?

    MRR takes the single highest ranked positive/relevant item (since a system always returns an ordered list, it is the first positive solution). Here depicted as rank_i. It does not care about other relevant items (e.g. at position 4 or 20)

    User MRR if:

    1. There is only one result
    2. You care only about the highest ranked result (web search scenario)

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%204.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%204.png)

    **Example:**

    ```xml
    Query: "Cities in California"

    Ranked results: "Portland", "Sacramento", "Los Angeles"

    Ranked results (binary relevance): [0, 1, 1]
    Number of correct answers possible: 2
    **Reciprocal Rank** = 1/2

    Precision at 1: 0/1
    Precision at 2: 1/2
    Precision at 3: 2/3
    **Average precision** = 1/m∗[1/2+2/3]=1/2∗[1/2+2/3]=0.38
    ```

    Source: [https://stats.stackexchange.com/questions/127041/mean-average-precision-vs-mean-reciprocal-rank](https://stats.stackexchange.com/questions/127041/mean-average-precision-vs-mean-reciprocal-rank)

- How does MRR differ to MAP?
    - MAP considers all relevant items in the returned ranking. MAP and MRR are equivalent if MAP is set to k=1, since it then looks only at the first relevant item, and so does MRR.

        Source:[https://stats.stackexchange.com/questions/127041/mean-average-precision-vs-mean-reciprocal-rank](https://stats.stackexchange.com/questions/127041/mean-average-precision-vs-mean-reciprocal-rank)

- What is the difference between implicit and explicit matrix factorization?
    - Implicit describes mostly unary ratings, such as that the user has seen an item or not (0 or 1)
    - Explicit desribes the actual given rating, such as 3.5 by a range from 0 to 5
- What is the median rank?
    - The position of the relevant item that is in the middle of the relevant item list (50%). If odd than it is one above.

    ```python
    Example:
    Relevant items position: [4, 60, 67, 77, 80]
    Median rank: 67
    ```

- What is Binary Cross-entropy?
    - With binary cross entropy I can only classify two classes, whereas categorical cross entropy can classify many classes.
- What algorithms can be used for recommendation?
    - Matrix Factorization (SVD
    - Nearest Neighbour (worse than MF)
        - LSTMS, provide seen movies as a list of Ids to a LSTM network. Also, add tags to it.

        ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%205.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%205.png)

        Source: [https://medium.com/deep-systems/movix-ai-movie-recommendations-using-deep-learning-5903d6a31607](https://medium.com/deep-systems/movix-ai-movie-recommendations-using-deep-learning-5903d6a31607)

- What is the Monte Carlo method?
    - random sampling
    - On wikipedia it is described as follows:
        1. Define a domain of possible inputs
        2. Generate inputs randomly from a [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution) over the domain
        3. Perform a [deterministic](https://en.wikipedia.org/wiki/Deterministic_algorithm) computation on the inputs
        4. Aggregate the results
    - Aggregating the result would be the mean for our P(x)

        ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%206.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%206.png)

    - Source: 
    [https://anotherdatum.com/vae.html](https://anotherdatum.com/vae.html)
    [https://en.wikipedia.org/wiki/Monte_Carlo_method](https://en.wikipedia.org/wiki/Monte_Carlo_method)
- What is the difference in the latent space of a variational autoencoder and a regular autoencoder?
    - AE learns just a compressed presentation of the input data under the restriction that it is able to reconstruct the input, whereas VAE assumes that the underlying input space has a probability distribution and the latent space itself is assumed as a probability distribution as well. (usually normal distribution). VAEs are penalized if the latent space distribution is divergent from the assumed prior (here normal distribution). Hence, VAEs are pushed torwards adjusting the distributions to the given prior distribution.
    - Source: [https://www.quora.com/What-is-the-difference-in-the-latent-space-of-a-variational-autoencoder-and-a-regular-autoencoder#:~:text=AutoEncoders are free to have,is as good as mine](https://www.quora.com/What-is-the-difference-in-the-latent-space-of-a-variational-autoencoder-and-a-regular-autoencoder#:~:text=AutoEncoders%20are%20free%20to%20have,is%20as%20good%20as%20mine).

## XAI Methods

- What is a partial dependence plot and how does it work?
    - Shows how the average prediction in your dataset changes when the j-th feature is changed
    - Requires to see the model as additive.
    - Computes the marginal effect on one or two features.

    Note:
    Marginal effect means, what one feature adds to the prediction if they others are independent. In order to calculate it we have to average over those non-interesting

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%207.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%207.png)

    Source:

     [https://christophm.github.io/interpretable-ml-book/pdp.html](https://christophm.github.io/interpretable-ml-book/pdp.html)

    This means the following:
    In order to make a statement about a variable and its partial dependency to the output we need to make the inferences with all possible values of the other features,
    resulting Here in B3,B3, C2,C3. At the end we have n predicted outcomes, which need to be
    averaged. This averaged y is the marginal contribution of A1.

    Example:

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%208.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%208.png)

    Source: [https://towardsdatascience.com/introducing-pdpbox-2aa820afd312](https://towardsdatascience.com/introducing-pdpbox-2aa820afd312)

- What are advantages of PDP?
    - perfect result, if features are independent
    - easy to implement
    - calculation is causal for model but not for the real world
- What are disadvantages of PDP?
    - max feature = 2
    - Some plots do not show the distribution → misleading interpretation
    - Assumption of independence (weight ↔ height example) → Solution: Accumulated Local Effect plots (ALE) // works with conditional distribution
    - Heterogeneous effects might be hidden. Cancelling out large positive and negative predictions (→ resulting in zero, meaning no effect on the model) → Solution: Individual Conditional Expectation Curves ⇒ ICE-Curves
- What is Individual Conditional Expectation plot?
    - Extension of Partidal Dependence Plot
- What is an additive model?
    - It is characterized that the effect of one variable does not depend on another (independence) and their contribution/effect adds up to the total effect/prediction. Therefore, we can assume to understand each variable individually (synonym marginally), and as a result, we are able to interpret the model.
    - Think of good old regression models. In a simple regression model, the effect of one variable does not depend on other variables and their effects add up to the total effect. If we can understand each variable individually or marginally, we would be able to interpret the entire model. However, this will no longer be true in the presence of interactions.

- What is the idea of Shapley Values?
    - Averaging over the contribution of all N! possible orderings. (Since non-linear models always depend on the order of the features)
- What is LAYER-WISE RELEVANCE PROPAGATION (LRP)?
    - It is similar to Saliency Maps as it produces a heatmap, but LRP connects it to the neurons that contributed the most.

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%209.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%209.png)

    Source: [https://dbs.uni-leipzig.de/file/XAI_Alexandra_Nau.pdf](https://dbs.uni-leipzig.de/file/XAI_Alexandra_Nau.pdf)

- What is the idea of a regularizer?
    - Discourage the NN memorizing/overfitting

- What is the manifold space?
    - A continuous, non-intersecting surface

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2010.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2010.png)

- What is the null hypothesis?
    - A null hypothesis is a hypothesis that says there is no statistical significance between the two variables in the hypothesis. It is the hypothesis that the researcher is trying to disprove. In the example, Susie's null hypothesis would be something like this: There is no statistically significant relationship between the type of water I feed the flowers and growth of the flowers. A researcher is challenged by the null hypothesis and usually wants to disprove it, to demonstrate that there is a statistically-significant relationship between the two variables in the hypothesis.
    Source: [https://study.com/academy/lesson/what-is-a-null-hypothesis-definition-examples.html](https://study.com/academy/lesson/what-is-a-null-hypothesis-definition-examples.html)
- What Is an Alternative Hypothesis?
    - An **alternative hypothesis** simply is the inverse, or opposite, of the null hypothesis. So, if we continue with the above example, the alternative hypothesis would be that there IS indeed a statistically-significant relationship between what type of water the flower plant is fed and growth. More specifically, here would be the null and alternative hypotheses for Susie's study:
    - **Null**: If one plant is fed club soda for one month and another plant is fed plain water, there will be no difference in growth between the two plants.
    - **Alternative**: If one plant is fed club soda for one month and another plant is fed plain water, the plant that is fed club soda will grow better than the plant that is fed plain water.
    - Source: [https://study.com/academy/lesson/what-is-a-null-hypothesis-definition-examples.html](https://study.com/academy/lesson/what-is-a-null-hypothesis-definition-examples.html)
- What is the Friedman's test?
    - Similar to the parametric repeated measures ANOVA, it is used to detect differences in treatments across multiple test attempts. Essentially, you have n data records, e.g. participants and those participants have experienced different types of something, e.g. wine or drugs. Now, what you're gonna do is to assign a rank to each entry. The goal is to find out whether there is a difference among those populations or not. At the end you either reject or you don't the null hypothesis.
    - Source: [https://www.statisticshowto.com/friedmans-test/](https://www.statisticshowto.com/friedmans-test/)
    - Source: [https://statistics.laerd.com/spss-tutorials/friedman-test-using-spss-statistics.php](https://statistics.laerd.com/spss-tutorials/friedman-test-using-spss-statistics.php)
- What is the Lower Bound?

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2011.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2011.png)

    Source: [https://www.mathsisfun.com/definitions/lower-bound.html](https://www.mathsisfun.com/definitions/lower-bound.html)

- What is Jensens inequality?

    Linear transformation of a variable followed by another linear transformation leads to the same result as the other way around.

    Example:

    ```python
    mean(f(x)) == f(mean(x)), for linear f()
    ```

    Jensens inequality says:

    ***The intuition of linear mappings does not hold for nonlinear functions.***

    Non linear functions are not a straight line when we think of the transformation on the input to the output. It is either convex (curving upward) or concarve (curving downwards).

    Example:

    ```python
    (x) == x^2 is an exponential convex function.
    ```

    I quote:

    *"Instead, the mean transform of an observation mean(f(x)) is always greater than the transform of the mean observation f(mean(x)), if the transform function is convex and the observations are not constant. We can state this as:"*

    ```python
    mean(f(x)) >= f(mean(x)), for convex f() and x is not a constant
    ```

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2012.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2012.png)

    Out intuition that the mean of a row of numbers multiplied by a non linear function (square), would be the same as applying the square to each of them and then calculate the mean, is wrong! The latter is greater.

    - used to make claims about a function where little is known about the distribution
    - used to compare arithmetic mean and geometric mean

    Source: [https://machinelearningmastery.com/a-gentle-introduction-to-jensens-inequality/](https://machinelearningmastery.com/a-gentle-introduction-to-jensens-inequality/)

- What is the geometric mean?
    - The Geometric Mean is a special type of average where we multiply the numbers together and then take a square root (for two numbers), cube root (for three numbers) etc.
    - For n numbers we take the nth root. (Nth root is concave)
- What is data scarcity?
    - "Data scarcity, means too few data points often because it is difficult to get data or the data is small as compared to the amount needed."

        Source: [https://www.quora.com/What-is-the-difference-between-data-scarcity-and-data-sparsity#:~:text=Data scarcity%2C means too few,space between your data points](https://www.quora.com/What-is-the-difference-between-data-scarcity-and-data-sparsity#:~:text=Data%20scarcity%2C%20means%20too%20few,space%20between%20your%20data%20points).

- What is a Cox Model?
    - Quote: "A Cox model is a statistical technique for exploring the
    relationship between the survival of a patient and several
    explanatory variables. Survival analysis is concerned with studying the time
    between entry to a study and a subsequent event (such as
    death)."

        Source: [http://www.bandolier.org.uk/painres/download/whatis/COX_MODEL.pdf](http://www.bandolier.org.uk/painres/download/whatis/COX_MODEL.pdf)

- Can a linear model describe a curve?

    Yes, that is possible. Usually a linear model only contains linear parameters and linear variables, but as long as the parameter remain linear and the variable to change is independent, it's exponent can be changed. For example can a independent variable be raised to quadratic, to fit a curve.

    Rule for a linear model:

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2013.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2013.png)

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2014.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2014.png)

    Source: [https://statisticsbyjim.com/regression/difference-between-linear-nonlinear-regression-models/](https://statisticsbyjim.com/regression/difference-between-linear-nonlinear-regression-models/)

- What are Restricted Boltzmann Machines?
    - Can be used for: dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling.
    - The goal is to learn a compressed version of features (lower dimensional representation) that describe the input well. The concept is similar to AEs but in contrary they don't have a dedicated decoding part. The decoding flows back to the input and updates the weights directly instead of having additional weights. The flown back reconstruction is compared and the weights updated.
    - Therefore, we need two biases. One is for the forward pass (hidden bias) and the other for the backward pass (reconstruction). The input layer is considered as visible layer and the second layer as Hidden Layer.

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2015.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2015.png)

    - RBM uses stochastic units with a particular distributions. Based on this we use Gibbs Sampling:

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2016.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2016.png)

    - As an alternative to backpropagation and we use Contrastive Divergence. (I guess it is due to the probability distribution. VAE solves this problem by the reparameterization trick)

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2017.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2017.png)

    Source: [https://medium.com/edureka/restricted-boltzmann-machine-tutorial-991ae688c154](https://medium.com/edureka/restricted-boltzmann-machine-tutorial-991ae688c154)

    Additional Source: [https://www.patrickhebron.com/learning-machines/week5.html](https://www.patrickhebron.com/learning-machines/week5.html)

- What is a Deep Belief Network?
    - Many layers of RBMs stacked together. For updating the weights, the Contrastive Divergense is still be used.
- What is synthetic data?
    - data generated by an algorithm
- What is the inverse covariance matrix?

    [https://www.quora.com/How-can-I-use-Gaussian-processes-to-perform-regression](https://www.quora.com/How-can-I-use-Gaussian-processes-to-perform-regression)

- What are explicit factor models?
    - Explicit factor models (EFM) [17] generate explanations based on the explicit features extracted from users’ reviews.
- What is Explainable Matrix Factorization (EMF)?
    - another algorithm that uses an explainability regularizer or soft constraint in the objective function of classical matrix factorization. The con- straining term tries to bring the user and explainable item’s latent factor vectors closer, thus favoring the appearance of explainable items at the top of the recommendation list.

    Source: An Explainable Autoencoder For Collaborative Filtering Recommendation ([https://arxiv.org/abs/2001.04344](https://arxiv.org/abs/2001.04344))

- What is the optimal transport cost (OT)?
    - It uses the Wasserstein distance to measure how much work has to be invested to transport one probability distribution to another. In other words: The cost to transform one distribution into the other.

     

- What is the infimum?
    - That is the lower bound (not to be mixed up with the minimum).
- What is the difference between Wasserstein and Kullback-Leibler?
    - Wasserstein is a metric and KL not (KL is not symmetric and does not satisfy the triangle inequality)
    - Intuition:
        - KL Divergence just tells you how similar two distributions are but Wasserstein distance gives you a measure for the effort to transport one probability mass to the other.

        ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2018.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2018.png)

        Source: [https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg](https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg)

- What is the chi-squared test?
    - used for categorical data
    - Does  one categorical variable affects another one?
    - calculates the p-value
    - In case of a low p-value:
        - Result is thought of as being "significant" meaning we think the variables are not independent.
    - chi square =  (Observed - Expected)^2 / Expected
    - Calculate the degree of freedom: Degree of Freedom = (rows − 1) × (columns − 1)
    - Look up p-value in pre-computed table by using chi-square and degree of freedom

        ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2019.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2019.png)

    - Source:
    [https://www.mathsisfun.com/data/chi-square-table.html](https://www.mathsisfun.com/data/chi-square-table.html)

- What is a CSR Matrix?

    Compressed sparse row (CSR), Algorithm to compress the size of a matrix by indexing the 1s with row_id, col_id

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2020.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2020.png)

    Source: [https://machinelearningmastery.com/sparse-matrices-for-machine-learning/](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)

- What is rocchio feedback?
    - Improving the search query by transforming the query and documents into a vector space and try to approach the centroid of the relevant documents while penalizing the distance to the centroid of the non-relevant documents.
    - Described as an equation:

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2021.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2021.png)

    - 

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2022.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2022.png)

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2023.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2023.png)

    Source and example calculation: [https://www.coursera.org/lecture/text-retrieval/lesson-5-2-feedback-in-vector-space-model-rocchio-PyTkW](https://www.coursera.org/lecture/text-retrieval/lesson-5-2-feedback-in-vector-space-model-rocchio-PyTkW)

- What is a Weighted Matrix Factorisation?

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2024.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2024.png)

    - In contrast to SVD that has a cost function that processes all in one (seen and unseen), weighted MF sums over observed and unobserved items separately. SVD leads to poor generalisation when the corpus is large due to a sparse matrix.

    ![6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2025.png](6%20XAI%200fe6f8c58d1e489ba64035eab21d66e2/Untitled%2025.png)

    - Matrix A is the interaction matrix.

        Source: [https://developers.google.com/machine-learning/recommendation/collaborative/matrix](https://developers.google.com/machine-learning/recommendation/collaborative/matrix)

- How can the objective function (cost) be minimized?
    - **[Stochastic gradient descent (SGD)](https://developers.google.com/machine-learning/crash-course/glossary#SGD)** is a generic method to minimize loss functions.
    - **Weighted Alternating Least Squares** (**WALS**) is specialized to this particular objective.

        Source: [https://developers.google.com/machine-learning/recommendation/collaborative/matrix](https://developers.google.com/machine-learning/recommendation/collaborative/matrix)

- What are the differences betweeen SGD and WALS?

    **SGD**

    +: **Very flexible—can use other loss functions.**

    +: **Can be parallelized.**

    -: **Slower—does not converge as quickly.**

    -: **Harder to handle the unobserved entries (need to use negative sampling or gravity).**

    **WALS**

    -: **Reliant on Loss Squares only.**

    +: **Can be parallelized.**

    +: **Converges faster than SGD.**

    +: **Easier to handle unobserved entries.**

- What are advantages and disadvantages of Collaborative Filtering?

    Taken from: [https://developers.google.com/machine-learning/recommendation/collaborative/summary](https://developers.google.com/machine-learning/recommendation/collaborative/summary)

    Advantages:

    **No domain knowledge necessary**

    We don't need domain knowledge because the embeddings are automatically learned.

    **Serendipity**

    The model can help users discover new interests. In isolation, the ML system may not know the user is interested in a given item, but the model might still recommend it because similar users are interested in that item.

    **Great starting point**

    To some extent, the system needs only the feedback matrix to train a matrix factorization model. In particular, the system doesn't need contextual features. In practice, this can be used as one of multiple candidate generators.

    Disadvantages:

    **Cannot handle fresh items**

    The prediction of the model for a given (user, item) pair is the dot product of the corresponding embeddings. So, if an item is not seen during training, the system can't create an embedding for it and can't query the model with this item. This issue is often called the **cold-start problem**. However, the following techniques can address the cold-start problem to some extent:

    - **Projection in WALS.** Given a new item i0 not seen in training, if the system has a few interactions with users, then the system can easily compute an embedding vi0 for this item without having to retrain the whole model. The system simply has to solve the following equation or the weighted version:

        minvi0∈Rd‖Ai0−Uvi0‖

        The preceding equation corresponds to one iteration in WALS: the user embeddings are kept fixed, and the system solves for the embedding of item i0. The same can be done for a new user.

    - **Heuristics to generate embeddings of fresh items.** If the system does not have interactions, the system can approximate its embedding by averaging the embeddings of items from the same category, from the same uploader (in YouTube), and so on.

    **Hard to include side features for query/item**

    **Side features** are any features beyond the query or item ID. For movie recommendations, the side features might include country or age. Including available side features improves the quality of the model. Although it may not be easy to include side features in WALS, a generalization of WALS makes this possible.

    To generalize WALS, **augment the input matrix with features** by defining a block matrix A¯, where:

    - Block (0, 0) is the original feedback matrix .

        A

    - Block (0, 1) is a multi-hot encoding of the user features.
    - Block (1, 0) is a multi-hot encoding of the item features.